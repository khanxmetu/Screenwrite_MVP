"""
Analyzer - Media-Driven Request Enhancement Engine

Enhances user requests by examining actual media content to extract specific details.
Works alongside Synth to transform vague requests into ultra-precise instructions
for the code generator by analyzing visual and audio content in media files.
"""

import re
from typing import Dict, Any, List, Optional
from google import genai
from google.genai import types


def extract_composition_state(tsx_code: Optional[str]) -> Dict[str, Any]:
    """
    Extract and filter composition to keep only video elements with their timing
    
    Returns:
    - total_duration: Duration from comments
    - filtered_tsx: TSX code with only video elements (preserves timing/positioning)
    - video_count: Number of video elements found
    """
    if not tsx_code:
        return {
            "total_duration": 0.0,
            "filtered_tsx": "",
            "video_count": 0
        }
    
    composition_state = {
        "total_duration": 0.0,
        "filtered_tsx": "",
        "video_count": 0
    }
    
    try:
        # Extract total duration from comments
        duration_patterns = [
            r'DURATION:\s*(\d+(?:\.\d+)?)',
            r'AI-determined Duration:\s*(\d+(?:\.\d+)?)\s*seconds'
        ]
        
        for pattern in duration_patterns:
            duration_match = re.search(pattern, tsx_code)
            if duration_match:
                composition_state["total_duration"] = float(duration_match.group(1))
                break
        
        # Find sequences that contain Video elements by using a different approach
        # Split the code into individual React.createElement blocks and filter for Video ones
        
        video_sequences = []
        
        # Look for React.createElement(Sequence blocks that contain Video
        sequence_starts = []
        for match in re.finditer(r'React\.createElement\(Sequence,', tsx_code):
            sequence_starts.append(match.start())
        
        for start_pos in sequence_starts:
            # Find the matching closing parenthesis for this Sequence
            paren_count = 0
            pos = start_pos
            sequence_start = start_pos
            
            while pos < len(tsx_code):
                char = tsx_code[pos]
                if char == '(':
                    paren_count += 1
                elif char == ')':
                    paren_count -= 1
                    if paren_count == 0:
                        # Found the complete sequence
                        sequence_text = tsx_code[sequence_start:pos + 1]
                        
                        # Check if this sequence contains a Video element
                        if 'React.createElement(Video,' in sequence_text:
                            video_sequences.append(sequence_text)
                            composition_state["video_count"] += 1
                        break
                pos += 1
        
        # Build the filtered TSX with only video elements
        if video_sequences:
            # Extract the header/setup part (imports, constants, etc.)
            header_parts = []
            
            # Get duration calculation if present
            duration_calc_match = re.search(r'const videoDurationInFrames.*?;', tsx_code)
            if duration_calc_match:
                header_parts.append(duration_calc_match.group(0))
            
            # Get fps reference if present
            fps_match = re.search(r'const \{ fps \}.*?;', tsx_code)
            if fps_match:
                header_parts.append(fps_match.group(0))
                
            # Get frame reference if present
            frame_match = re.search(r'const frame.*?;', tsx_code)
            if frame_match:
                header_parts.append(frame_match.group(0))
            
            # Build the filtered composition
            filtered_parts = []
            if header_parts:
                filtered_parts.extend(header_parts)
                filtered_parts.append("")  # Empty line
            
            filtered_parts.append("return React.createElement(AbsoluteFill, {},")
            
            # Add each video sequence with proper indentation
            for i, video_seq in enumerate(video_sequences):
                if i > 0:
                    filtered_parts.append(",")
                filtered_parts.append(f"  {video_seq}")
            
            filtered_parts.append(");")
            
            composition_state["filtered_tsx"] = "\n".join(filtered_parts)
        
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to filter composition: {e}")
    
    return composition_state


class AnalysisResult:
    """Result from the Analyzer enhancement engine"""
    def __init__(
        self, 
        enhanced_request: str,
        media_analysis: Dict[str, Any] = None,
        visual_context: Dict[str, Any] = None
    ):
        self.enhanced_request = enhanced_request
        self.media_analysis = media_analysis or {}
        self.visual_context = visual_context or {}


async def analyze_content(
    enhanced_request: str,
    media_for_analysis: List[str],
    media_library: List[Dict[str, Any]],
    preview_frame: Optional[str],
    current_composition: Optional[str],
    gemini_api: Any
) -> AnalysisResult:
    """
    Enhance user request by analyzing media content and extracting specific details
    
    Similar to Synth, this function takes a request and makes it more actionable 
    for the code generator by examining actual media content to replace vague 
    references with precise specifications.
    
    Args:
        enhanced_request: The request enhanced by Synth
        media_for_analysis: List of media file names to analyze  
        media_library: Full media library with metadata
        preview_frame: Base64 encoded screenshot for visual analysis
        current_composition: Current TSX composition code for timing context
        gemini_api: Gemini AI client
        
    Returns:
        AnalysisResult with media-enhanced request and analysis context
    """
    
    print(f"üîç Analyzer: Processing {len(media_for_analysis)} media files")
    
    # Extract basic composition info without filtering
    total_duration = 0.0
    if current_composition:
        # Extract duration from comments
        duration_patterns = [
            r'DURATION:\s*(\d+(?:\.\d+)?)',
            r'AI-determined Duration:\s*(\d+(?:\.\d+)?)\s*seconds'
        ]
        
        for pattern in duration_patterns:
            duration_match = re.search(pattern, current_composition)
            if duration_match:
                total_duration = float(duration_match.group(1))
                break
    
    print(f"üìê Composition duration: {total_duration}s")
    
    if not media_for_analysis:
        # No media to analyze, return as-is
        return AnalysisResult(
            enhanced_request=enhanced_request,
            media_analysis={"status": "no_media_to_analyze"},
            visual_context={"status": "no_media_to_analyze"}
        )
    
    # Find Gemini file IDs for the media files to analyze
    gemini_file_refs = []
    for media_name in media_for_analysis:
        # Find the media item in the library
        media_item = next((item for item in media_library if item.get('name') == media_name), None)
        if media_item and media_item.get('gemini_file_id'):
            gemini_file_refs.append({
                'name': media_name,
                'gemini_file_id': media_item['gemini_file_id']
            })
            print(f"üìÅ Found Gemini file ID for {media_name}: {media_item['gemini_file_id']}")
        else:
            print(f"‚ö†Ô∏è No Gemini file ID found for {media_name}")
    
    if not gemini_file_refs:
        print("‚ùå No Gemini file references found for analysis")
        return AnalysisResult(
            enhanced_request=enhanced_request,
            media_analysis={"status": "no_gemini_files"},
            visual_context={"status": "no_gemini_files"}
        )
    
    try:
        # System instruction for the analyzer role
        system_instruction = """You are a content-based request enhancement engine. Your ONLY job is to resolve ambiguities that require understanding what's actually happening inside the media content.

‚ö†Ô∏è CRITICAL: Your output goes directly to a code generator. You only handle requests that depend on visual/audio content analysis."""

        # Build analysis prompt with composition context
        analysis_prompt = f"""USER REQUEST: "{enhanced_request}"

COMPOSITION CONTEXT:
- Total composition duration: {total_duration} seconds  
- Current composition code:
{current_composition or "No composition provided"}

CONTENT-BASED ENHANCEMENT OBJECTIVE:
Examine the media files and resolve content-dependent references in the user request. Replace content-based ambiguities with specific details extracted from the media.

EXAMPLES OF WHAT YOU HANDLE:
- "when the chimpanzee appears" ‚Üí "at 23.4 seconds when the chimpanzee is visible"
- "match the dominant color" ‚Üí "use color #3A7BD4 (the dominant blue from the sky)"
- "complement the mood" ‚Üí "use warm orange tones (#FF8C42) to complement the sunset atmosphere"
- "where the text is readable" ‚Üí "position at top-left to avoid the bright water area"
- "don't cover the person" ‚Üí "position at bottom-right since person is centered in frame"
- "place it where there's space" ‚Üí "position at 25% from left, 80% from top in the dark forest area"
- "avoid the action" ‚Üí "position at top-center since the car chase happens in bottom half"
- "where it won't interfere" ‚Üí "position at 15%, 20% to avoid the bird flying across center"

CRITICAL: VIDEO-TO-COMPOSITION TIMING CONVERSION
Follow these exact reasoning patterns when analyzing video content and converting timing to composition coordinates.

‚ö†Ô∏è REASONING INSTRUCTION: Use the step-by-step Q&A format shown below. Break down each problem systematically, check video segment ranges, and calculate precise timing offsets. This is exactly how you should reason through timing conversions.

Use these Q&A reasoning patterns with real generated compositions:

EXAMPLE 1 - Reordered Video Segments (Real Generated Code):
```
// Generated at: 2025-08-21 23:40:20
// User Request: Analyze the video to identify when the bird appears and add text...

const videoSrc = 'blob:http://localhost:5173/2ffc470e-70f9-429b-b095-734b7aeccc73';
const videoTotalDurationSeconds = 172.332063;
const videoSegment1StartSeconds = 30;
const videoSegment1EndSeconds = videoTotalDurationSeconds;
const videoSegment2StartSeconds = 0;
const videoSegment2EndSeconds = 30;

const segment1DurationFrames = (videoSegment1EndSeconds - videoSegment1StartSeconds) * fps;
const segment2DurationFrames = (videoSegment2EndSeconds - videoSegment2StartSeconds) * fps;

React.createElement(Sequence, {{
  from: 0,
  durationInFrames: segment1DurationFrames,
  children: React.createElement(Video, {{
    startFrom: videoSegment1StartSeconds * fps,  // 30s
    endAt: videoSegment1EndSeconds * fps,        // 172.332063s
  }})
}}),
React.createElement(Sequence, {{
  from: segment1DurationFrames,
  durationInFrames: segment2DurationFrames,
  children: React.createElement(Video, {{
    startFrom: videoSegment2StartSeconds * fps,  // 0s
    endAt: videoSegment2EndSeconds * fps,        // 30s
  }})
}})
```
Q: The user wants me to add text when a bird appears. I found the bird at 25s in the original video. This composition reorders the video - when should the text appear?
A: Step 1: Identify bird timing in original video ‚Üí 25s
Step 2: Check which segment contains 25s:
   - Segment 1: videoSegment1StartSeconds (30s) to videoSegment1EndSeconds (172.332063s) ‚Üí doesn't contain 25s
   - Segment 2: videoSegment2StartSeconds (0s) to videoSegment2EndSeconds (30s) ‚Üí 25s falls here ‚úì
Step 3: Calculate offset within segment 2 ‚Üí 25s - videoSegment2StartSeconds = 25s - 0s = 25s
Step 4: Find segment 2's composition start ‚Üí from: segment1DurationFrames
Step 5: Convert to frames ‚Üí segment1DurationFrames + (25s * fps)
Step 6: Set birdTextFromFrame = segment1DurationFrames + (25 - videoSegment2StartSeconds) * fps
Answer: Text should appear at birdTextFromFrame = segment1DurationFrames + (25 * fps) frames in composition.

EXAMPLE 2 - Simple Video with Text Overlays (Real Generated Code):
```
// Generated at: 2025-08-21 22:46:27
// User Request: Add text when an eagle appears in the nature video...
// AI-determined Duration: 20.04 seconds

const videoDurationInFrames = Math.ceil(20.04 * fps);

React.createElement(Sequence, {{
  from: 0,
  durationInFrames: videoDurationInFrames,
  children: React.createElement(Video, {{
    src: 'blob:http://localhost:5173/a2a9ff8f-faf7-4102-972f-d9ad42da7316',
    startFrom: 0,  // Video plays from 0s
    endAt: 20.04 * fps  // to 20.04s
  }})
}}),

// Text 1: 'Black Screen'
React.createElement(Sequence, {{
  from: 0,
  durationInFrames: 1 * fps,
  children: React.createElement('div', {{
    style: {{ position: 'absolute', top: '80%', left: '50%' }}
  }}, 'Black Screen')
}}),

// Text 2: 'Sunlit Purple Flowers'  
React.createElement(Sequence, {{
  from: 1 * fps,
  durationInFrames: (9 - 1) * fps,
  children: React.createElement('div', {{
    style: {{ position: 'absolute', top: '80%', left: '50%' }}
  }}, 'Sunlit Purple Flowers')
}})
```
Q: The user wants me to add text when an eagle appears. I found the eagle at 15s in the original video. The video plays normally from 0s. Where should I position the text to avoid existing overlays?
A: Step 1: Identify eagle timing in original video ‚Üí 15s
Step 2: Check video segment range ‚Üí Simple video from 0s to 20.04s, so 15s is included ‚úì  
Step 3: Calculate composition timing ‚Üí 15s occurs at 15s in composition (no offset)
Step 4: Check existing text positions at 15s:
   - At 15s: "Sunlit Purple Flowers" text is active (from 1s to 9s) ‚Üí No, it's inactive
   - No active text overlays at 15s
Step 5: Choose safe positioning ‚Üí Can use standard positions
Step 6: Set eagleTextFromFrame = 15 * fps
Answer: Text should appear at eagleTextFromFrame = 15 * fps, positioned at top: '20%' to avoid future text conflicts.

EXAMPLE 3 - Animation with Frame Calculations (Real Generated Code):
```
// Generated at: 2025-08-21 13:03:36
// User Request: Add dramatic text when car crash happens at 2.5s...
// AI-determined Duration: 4.333333333333333 seconds

const frame = useCurrentFrame();
const {{ fps }} = useVideoConfig();

const videoDurationInFrames = Math.ceil(4.333333333333333 * fps);

React.createElement(Sequence, {{
  from: 0,
  durationInFrames: videoDurationInFrames,
  children: React.createElement(Video, {{
    src: 'blob:http://localhost:5173/crash-video',
    startFrom: 0,  // Video plays from 0s
    endAt: 4.333333333333333 * fps  // to 4.33s
  }})
}}),

const crashWarningFromFrame = 2.0 * fps;  // 0.5s before crash at 2.5s
const crashWarningDurationFrames = 1.0 * fps;

React.createElement(Sequence, {{
  from: crashWarningFromFrame,
  durationInFrames: crashWarningDurationFrames,
  children: (() => {{
    const localFrame = frame - crashWarningFromFrame;
    const textOpacity = interpolate(localFrame, [0, 10], [0, 1]);
    return React.createElement('div', {{
      style: {{ opacity: textOpacity }}
    }}, 'WARNING: CRASH!');
  }})()
}})
```
Q: The user wants me to add text when a car crash happens. I found the crash at 2.5s in the original video. The video plays normally. How should I time the text to appear just before the crash for dramatic effect?
A: Step 1: Identify crash timing in original video ‚Üí 2.5s
Step 2: Convert to frames ‚Üí 2.5s * fps = 75 frames (assuming 30fps)
Step 3: Calculate dramatic timing ‚Üí Appear 0.5s before crash = 2.0s = 60 frames
Step 4: Set text duration ‚Üí Show for 1s through the crash = 30 frames duration
Step 5: Set crashWarningFromFrame = (2.0 * fps) and crashWarningDurationFrames = (1.0 * fps)
Step 6: Add fade-in animation ‚Üí interpolate(localFrame, [0, 10], [0, 1]) for first 10 frames
Answer: Use crashWarningFromFrame = 60 frames, crashWarningDurationFrames = 30 frames, with fade-in over first 10 frames.

EXAMPLE 4 - Content Not Included Analysis:
```
// Generated composition with limited video segment
const videoSegment1StartSeconds = 50;
const videoSegment1EndSeconds = 150;

React.createElement(Sequence, {{
  from: 0,
  durationInFrames: (150 - 50) * fps,
  children: React.createElement(Video, {{
    startFrom: 50 * fps,  // Only plays 50s-150s of original video
    endAt: 150 * fps
  }})
}})
```
Q: The user wants me to add text when a sunset appears. I found sunset at 180s in the original video. The composition has videoSegment1StartSeconds = 50s and videoSegment1EndSeconds = 150s. Can I add the sunset text?
A: Step 1: Identify sunset timing in original video ‚Üí 180s
Step 2: Check video segment range ‚Üí 50s to 150s in original video
Step 3: Check if sunset is included ‚Üí 180s > 150s (sunset occurs AFTER segment ends)
Step 4: Determine availability ‚Üí Sunset timing not included in composition
Answer: Cannot add sunset text - the sunset at 180s occurs after this composition's video segment which ends at 150s.

EXAMPLE 5 - Position Analysis with Existing Elements:
```
// Composition with multiple existing text overlays
React.createElement(Sequence, {{
  from: 0,
  children: React.createElement('div', {{
    style: {{ position: 'absolute', top: '80%', left: '25%' }}
  }}, 'Existing Text 1')
}}),
React.createElement(Sequence, {{
  from: 30,
  children: React.createElement('div', {{
    style: {{ position: 'absolute', top: '80%', left: '75%' }}
  }}, 'Existing Text 2')
}})
```
Q: The user wants me to position text where it won't interfere with existing elements. I analyzed the composition and found multiple text overlays at 80% top. Where should I place new text?
A: Step 1: Analyze existing text positions ‚Üí Multiple elements at top: '80%'
Step 2: Identify safe zones ‚Üí Top area (0%-60%), bottom area (90%-100%), side areas
Step 3: Check for visual conflicts ‚Üí Avoid crowded 80% area
Step 4: Select optimal position ‚Üí top: '20%' for clear separation from existing 80% elements  
Step 5: Consider content behind ‚Üí Ensure good contrast with video content
Answer: Position new text at top: '20%', left: '50%' with transform: 'translateX(-50%)' for center alignment, avoiding the crowded 80% area.

OUTPUT FORMAT: Return the enhanced request with content-based ambiguities resolved using specific details from the media analysis, with all timing converted to composition seconds.

‚ö†Ô∏è CRITICAL: Your output must be clearly actionable for a Remotion LLM code generator:
- Use precise technical specifications (hex colors, exact coordinates, frame numbers)
- Provide concrete measurements and timing values
- Include all necessary parameters for immediate Remotion component implementation
- Ensure the request can be executed without further clarification or analysis

ENHANCED OUTPUT EXAMPLE:
Input: "add text when the sunset looks beautiful"
Output: "Add text overlay with content 'Golden Hour Magic', fontSize 56px, fontFamily 'Inter', color #FFFFFF, backgroundColor rgba(255,140,66,0.8), positioned at 50% left, 15% top, appearing from frame 1080 to frame 1380 (36.0s to 46.0s in composition) when the sunset reaches peak orange saturation #FF8C42 in the sky"

Return ONLY the enhanced request as a single, comprehensive instruction."""

        # Prepare content parts based on file type (file ID vs Cloud Storage URI)
        content_parts = []
        for file_ref in gemini_file_refs:
            file_id = file_ref['gemini_file_id']
            if file_id.startswith('gs://'):
                # Vertex AI: Use Cloud Storage URI
                from google.genai.types import Part
                # Try to get MIME type from the file reference or default to video
                mime_type = file_ref.get('mime_type', 'video/mp4')
                content_parts.append(Part.from_uri(file_uri=file_id, mime_type=mime_type))
            else:
                # Standard API: Use Files API
                content_parts.append(gemini_api.files.get(name=file_id))
        
        # Call Gemini for analysis
        response = gemini_api.models.generate_content(
            model="gemini-2.5-flash",
            contents=[
                *content_parts,
                analysis_prompt
            ],
            config=types.GenerateContentConfig(
                system_instruction=system_instruction,
                temperature=0.4
            )
        )
        
        enhanced_final_request = response.text.strip()
        
        print(f"‚úÖ Analyzer: Enhanced request: '{enhanced_final_request[:100]}...'")
        
        return AnalysisResult(
            enhanced_request=enhanced_final_request,
            media_analysis={
                "analyzed_files": [ref['name'] for ref in gemini_file_refs],
                "gemini_files_used": [ref['gemini_file_id'] for ref in gemini_file_refs],
                "composition_duration": total_duration,
                "status": "analyzed"
            },
            visual_context={
                "files_processed": len(gemini_file_refs),
                "composition_context_used": True,
                "composition_duration": total_duration,
                "status": "completed"
            }
        )
        
    except Exception as e:
        print(f"‚ùå Analyzer: Failed to analyze media - {str(e)}")
        
        # Return original request if analysis fails
        return AnalysisResult(
            enhanced_request=enhanced_request,
            media_analysis={"status": "analysis_failed", "error": str(e)},
            visual_context={"status": "analysis_failed", "error": str(e)}
        )
